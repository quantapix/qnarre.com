<div class="container slogan-points px-4 px-md-3">
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white bg-danger">
        {{ partial "icons/code" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">Detect Direct Contradictions</h2>
      <p class="lead fw-normal">Consistency is simple, contradictions are limitless. Direct textual contradictions should be detected quickly.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/modeling/">Representation</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"The feature extractor transfers the
text input into a feature vector. When
using word embeddings, it is possible
for words with similar meaning to have a
similar representation. Classifiers can
then selectively extrapolate the results
to sentences and paragraphs.”</pre>
    </div>
  </section>
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white bg-success">
        {{ partial "icons/collapse" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">Infer Indirect Contradictions</h2>
      <p class="lead fw-normal">False conjectures lead to indirect contradictions. Weighing the cumulative "credibility" of conjectures allows us to infer indirect contradictions.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/modeling/">Preparation</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"Sentiment analysis is the process of
detecting positive or negative sentiment
in text. All utterances are uttered in
context. Analyzing subjective sentiment
without context is hard. For bipolar
text, context is learned by training or 
fine-tuning on both 'poles'."</pre>
    </div>
  </section>
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white bg-warning">
        {{ partial "icons/expand" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">"Conflict Graph" Of Contradictions</h2>
      <p class="lead fw-normal">Connecting all the contradictions in a domain results in its perhaps intended "conflict graph". Credibility weights of edges gives us the quantitative means for analysis.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/analytics/">Relationships</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"An inferred conflict graph is most
effectively modeled by an ER model (or
entity-relationship model). Granularity
of such a model is based on learned and
inferred representations allowing for
later refinements and even versioning."</pre>
    </div>
  </section>
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white qal-bg-purple-bright">
        {{ partial "icons/droplet-fill" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">Infer Implicit Context</h2>
      <p class="lead fw-normal">Natural languages rely extensively on the "understood" or implicit context of sentences. Inferring elements of this context (from "parts of speech" statistical analysis) allows us to "fill in" the variables of the lexical scopes of our sentences.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/semantics/">Contexts</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"Part-of-speech (POS) tagging categorizes
words in text depending on the definition
of the word and its context. Generated ER
models augment such inference mechanisms
to create necessary 'contexts' on the fly."</pre>
    </div>
  </section>
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white bg-danger">
        {{ partial "icons/circle-square" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">Construct Dependency Graphs</h2>
      <p class="lead fw-normal">Compilers successfully analyze dependency graphs of variables for optimization. Natural language text is already optimized, and we need to reverse engineer it to arrive to an explicit, or "precise", formulation of sentences.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/semantics/">Synthesis</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"Inferred contexts of sentences can be
expanded to paragraphs and even documents
using efficient and proven RDBMS 'tricks'
and on-demand synthesized SQL."</pre>
    </div>
  </section>
  <section class="row mb-5 pb-md-4 align-items-center">
    <div class="col-md-5">
      <div class="slogan-points-icon d-inline-block mb-2 text-white bg-success">
        {{ partial "icons/code" (dict "width" "32" "height" "32") }}
      </div>
      <h2 class="display-5 fw-normal">Train With 'Annotated Reality'</h2>
      <p class="lead fw-normal">"Parts of speech" decomposition of sentences allows us to greatly expand our corpus with labeled samples. Targeted explicit contradictions can be introduced to guide the "attention" mechanism of our subsequent training sessions.</p>
      <a class="btn btn-lg btn-outline-primary mb-3" href="/reports/">Reports</a>
    </div>
    <div class="col-md-7 ps-md-5">
      <pre>
"Iterations of the training, inference
and ER synthesis loops create the same
dynamic that gave us the success of deep
learning neural networks, with the added
benefits of quantitative versioning.”</pre>
    </div>
  </section>
</div>
